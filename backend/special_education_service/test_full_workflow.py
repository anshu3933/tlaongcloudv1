#!/usr/bin/env python3
"""
üîÑ Complete End-to-End Workflow Test

Tests the entire pipeline:
1. Frontend ‚Üí Backend message passing with document_id
2. Assessment data bridge retrieval 
3. LLM prompt enhancement with real data
4. Response flattener processing
5. Frontend-ready structured output
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path
from uuid import UUID, uuid4

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.services.iep_service import IEPService
from src.repositories.iep_repository import IEPRepository
from src.repositories.assessment_repository import AssessmentRepository
from src.utils.gemini_client import GeminiClient
from src.database import get_db_session, create_tables
from src.utils.response_flattener import SimpleIEPFlattener

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

async def test_full_workflow():
    """Test complete end-to-end workflow"""
    
    logger.info("üîÑ Starting Complete End-to-End Workflow Test")
    
    # Initialize database
    await create_tables()
    
    async with get_db_session() as session:
        # Initialize repositories
        iep_repo = IEPRepository(session)
        assessment_repo = AssessmentRepository(session)
        
        # Test student and document IDs
        test_student_id = UUID("c6f74363-c1fb-4b0f-bd6b-0ae5c8a6f826")
        test_document_id = str(uuid4())
        
        logger.info(f"üìã Test setup - Student: {test_student_id}")
        logger.info(f"üìÑ Test setup - Document: {test_document_id}")
        
        # Step 1: Create mock assessment data (simulating Document AI)
        logger.info("üìÑ Step 1: Creating mock assessment data...")
        
        mock_extracted_data = {
            "document_id": UUID(test_document_id),
            "structured_data": {
                "educational_objectives": [
                    {
                        "area": "Reading Comprehension",
                        "current_performance": "Student reads at 2nd grade level with 60% comprehension on grade-level passages",
                        "goal": "Improve reading comprehension to 3rd grade level with 80% accuracy",
                        "measurable_outcome": "80% accuracy on 3rd grade level comprehension questions",
                        "support_needed": "Small group instruction with graphic organizers and visual supports"
                    },
                    {
                        "area": "Mathematical Problem Solving", 
                        "current_performance": "Struggles with multi-step word problems, demonstrates strength in single-step computation",
                        "goal": "Solve 2-step word problems with 75% accuracy",
                        "measurable_outcome": "Correct solution process and answer on 75% of 2-step problems",
                        "support_needed": "Step-by-step problem solving templates and manipulatives"
                    }
                ],
                "performance_levels": {
                    "reading": {
                        "current_level": "Below grade level - reading at early 2nd grade level, requires intensive intervention"
                    },
                    "mathematics": {
                        "current_level": "Grade-appropriate computation skills, below grade level in problem solving applications"
                    }
                },
                "strengths": [
                    "Strong visual processing abilities",
                    "Excellent mathematical computation skills", 
                    "Responds well to structured, predictable routines",
                    "Demonstrates persistence with challenging tasks when provided appropriate supports"
                ],
                "areas_of_concern": [
                    "Reading fluency and comprehension significantly below grade level",
                    "Difficulty with abstract mathematical reasoning",
                    "Challenges with written expression and organization",
                    "Requires additional processing time for complex multi-step tasks"
                ],
                "recommendations": [
                    "Provide explicit, systematic reading instruction using evidence-based programs",
                    "Use visual supports and graphic organizers for comprehension tasks",
                    "Implement step-by-step problem solving frameworks for mathematics",
                    "Allow extended time for processing and completing complex tasks",
                    "Provide frequent positive reinforcement and break complex tasks into smaller components"
                ]
            },
            "extraction_confidence": 0.89,
            "extraction_method": "google_document_ai",
            "completeness_score": 0.94
        }
        
        try:
            created_extracted = await assessment_repo.create_extracted_assessment_data(mock_extracted_data)
            logger.info(f"‚úÖ Created mock extracted data: {created_extracted['id']}")
        except Exception as e:
            logger.error(f"‚ùå Error creating extracted data: {e}")
            return False
        
        # Create mock test scores
        mock_scores = [
            {
                "document_id": UUID(test_document_id),
                "test_name": "WISC-V",
                "subtest_name": "Verbal Comprehension Index",
                "standard_score": 85,
                "percentile_rank": 16,
                "score_type": "composite",
                "extraction_confidence": 0.92
            },
            {
                "document_id": UUID(test_document_id),
                "test_name": "WIAT-IV", 
                "subtest_name": "Reading Comprehension",
                "standard_score": 78,
                "percentile_rank": 7,
                "score_type": "subtest",
                "extraction_confidence": 0.88
            },
            {
                "document_id": UUID(test_document_id),
                "test_name": "WIAT-IV",
                "subtest_name": "Mathematical Problem Solving", 
                "standard_score": 82,
                "percentile_rank": 12,
                "score_type": "subtest",
                "extraction_confidence": 0.86
            }
        ]
        
        for score_data in mock_scores:
            try:
                created_score = await assessment_repo.create_psychoed_score(score_data)
                logger.info(f"‚úÖ Created test score: {score_data['test_name']} - {score_data['subtest_name']}")
            except Exception as e:
                logger.error(f"‚ùå Error creating test score: {e}")
        
        await session.commit()
        logger.info("‚úÖ Mock assessment data created successfully")
        
        # Step 2: Test Assessment Data Bridge
        logger.info("üîó Step 2: Testing Assessment Data Bridge...")
        
        try:
            gemini_client = GeminiClient()
            # Create a simple IEP service without full dependencies for testing
            class TestIEPService:
                def __init__(self, repo):
                    self.repository = repo
                    
                async def _fetch_assessment_data(self, document_id: str, student_id: UUID):
                    """Copy of the assessment bridge method for testing"""
                    from src.repositories.assessment_repository import AssessmentRepository
                    assessment_repo = AssessmentRepository(self.repository.session)
                    
                    assessment_data = {
                        "test_scores": [],
                        "composite_scores": {},
                        "educational_objectives": [],
                        "recommendations": [],
                        "assessment_confidence": 0.0
                    }
                    
                    # Fetch extracted data
                    try:
                        from uuid import UUID as ConvertUUID
                        doc_uuid = ConvertUUID(document_id)
                        extracted_data = await assessment_repo.get_document_extracted_data(doc_uuid)
                        
                        if extracted_data and extracted_data.get("structured_data"):
                            structured = extracted_data["structured_data"]
                            
                            if structured.get("educational_objectives"):
                                assessment_data["educational_objectives"] = structured["educational_objectives"]
                            
                            if structured.get("recommendations"):
                                assessment_data["recommendations"] = structured["recommendations"]
                                
                            assessment_data["assessment_confidence"] = extracted_data.get("extraction_confidence", 0.0)
                    
                    except Exception as e:
                        logger.error(f"Error fetching extracted data: {e}")
                    
                    # Fetch test scores
                    try:
                        scores = await assessment_repo.get_document_psychoed_scores(ConvertUUID(document_id))
                        assessment_data["test_scores"] = scores
                    except Exception as e:
                        logger.error(f"Error fetching test scores: {e}")
                    
                    return assessment_data
            
            test_service = TestIEPService(iep_repo)
            bridge_data = await test_service._fetch_assessment_data(test_document_id, test_student_id)
            
            logger.info("üìä Assessment Bridge Results:")
            logger.info(f"  Test Scores: {len(bridge_data.get('test_scores', []))}")
            logger.info(f"  Educational Objectives: {len(bridge_data.get('educational_objectives', []))}")
            logger.info(f"  Recommendations: {len(bridge_data.get('recommendations', []))}")
            logger.info(f"  Confidence: {bridge_data.get('assessment_confidence', 0):.2f}")
            
            if bridge_data.get('test_scores'):
                logger.info("‚úÖ Assessment bridge successfully retrieved test scores")
            else:
                logger.warning("‚ö†Ô∏è Assessment bridge did not retrieve test scores")
                
        except Exception as e:
            logger.error(f"‚ùå Assessment bridge test failed: {e}")
            return False
        
        # Step 3: Test LLM Prompt Enhancement
        logger.info("ü§ñ Step 3: Testing LLM Prompt Enhancement...")
        
        try:
            # Test the prompt formatting function
            student_data_with_assessment = {
                "student_id": str(test_student_id),
                "student_name": "Test Student",
                "grade_level": "3rd Grade",
                **bridge_data
            }
            
            formatted_prompt_section = gemini_client._format_assessment_data_for_prompt(student_data_with_assessment)
            
            logger.info(f"üìÑ Formatted prompt section length: {len(formatted_prompt_section)} characters")
            
            # Check for key evidence in prompt
            evidence_checks = [
                ("Test scores", "85" in formatted_prompt_section or "78" in formatted_prompt_section),
                ("Test names", "WISC-V" in formatted_prompt_section or "WIAT-IV" in formatted_prompt_section),
                ("Educational objectives", "Reading Comprehension" in formatted_prompt_section),
                ("Confidence score", str(bridge_data.get('assessment_confidence', 0)) in formatted_prompt_section),
                ("Recommendations", "visual supports" in formatted_prompt_section.lower())
            ]
            
            logger.info("üîç Evidence in LLM prompt:")
            for check_name, found in evidence_checks:
                status = "‚úÖ" if found else "‚ùå"
                logger.info(f"  {status} {check_name}: {'Found' if found else 'Not found'}")
            
            evidence_found = sum(1 for _, found in evidence_checks if found)
            if evidence_found >= 3:
                logger.info(f"‚úÖ LLM prompt enhancement working: {evidence_found}/5 evidence types found")
            else:
                logger.warning(f"‚ö†Ô∏è LLM prompt may need improvement: only {evidence_found}/5 evidence types found")
                
        except Exception as e:
            logger.error(f"‚ùå LLM prompt test failed: {e}")
            return False
        
        # Step 4: Test Response Flattener
        logger.info("üîß Step 4: Testing Response Flattener...")
        
        try:
            # Create mock complex IEP response to test flattener
            mock_iep_response = {
                "id": str(uuid4()),
                "student_id": str(test_student_id),
                "content": {
                    "student_info": {"name": "Test Student", "grade": "3rd Grade"},
                    "long_term_goal": "Student will demonstrate grade-level proficiency...",
                    "services": {
                        "services": {
                            "special_education": "Resource room support 5x weekly",
                            "related_services": ["Speech therapy", "OT consultation"]
                        }
                    },
                    "present_levels": {
                        "present_levels": "Based on assessments, student performs below grade level..."
                    },
                    "goals": [
                        {"domain": "Reading", "goal": "Improve comprehension to 80% accuracy"},
                        {"domain": "Math", "goal": "Solve 2-step problems with 75% accuracy"}
                    ]
                }
            }
            
            # Apply flattener
            flattened_response = SimpleIEPFlattener.flatten_for_frontend(mock_iep_response)
            
            logger.info("üìä Flattener Results:")
            logger.info(f"  Original size: {len(str(mock_iep_response))} characters")
            logger.info(f"  Flattened size: {len(str(flattened_response))} characters") 
            
            # Check for [object Object] prevention
            flattened_str = json.dumps(flattened_response)
            has_object_object = "[object Object]" in flattened_str
            
            if not has_object_object:
                logger.info("‚úÖ Response flattener successfully prevents [object Object] errors")
            else:
                logger.error("‚ùå Response flattener failed - [object Object] found in output")
                return False
            
            # Check that complex structures were flattened
            content = flattened_response.get("content", {})
            services_type = type(content.get("services", "")).__name__
            goals_type = type(content.get("goals", "")).__name__
            
            logger.info(f"  Services field type: {services_type}")
            logger.info(f"  Goals field type: {goals_type}")
            
            if services_type == "str" and goals_type == "str":
                logger.info("‚úÖ Complex nested structures successfully flattened to strings")
            else:
                logger.warning("‚ö†Ô∏è Some complex structures may not be fully flattened")
                
        except Exception as e:
            logger.error(f"‚ùå Response flattener test failed: {e}")
            return False
        
        # Step 5: Test Complete API Response Structure
        logger.info("üì° Step 5: Testing Complete API Response...")
        
        try:
            # Simulate what frontend would receive
            api_response = flattened_response
            content = api_response.get("content", {})
            
            logger.info("üìã Final API Response Structure:")
            logger.info(f"  Response keys: {list(api_response.keys())}")
            logger.info(f"  Content keys: {list(content.keys())}")
            logger.info(f"  Content types: {[(k, type(v).__name__) for k, v in content.items()]}")
            
            # Check frontend compatibility
            frontend_safe = True
            problematic_fields = []
            
            for key, value in content.items():
                if isinstance(value, (dict, list)) and len(str(value)) > 100:
                    frontend_safe = False
                    problematic_fields.append(f"{key} ({type(value).__name__})")
            
            if frontend_safe:
                logger.info("‚úÖ Response is frontend-safe - no complex objects that would show as [object Object]")
            else:
                logger.warning(f"‚ö†Ô∏è Potential frontend display issues in: {', '.join(problematic_fields)}")
            
            # Test JSON serialization (frontend requirement)
            try:
                json_str = json.dumps(api_response)
                logger.info(f"‚úÖ Response is JSON serializable ({len(json_str)} characters)")
            except Exception as json_error:
                logger.error(f"‚ùå Response not JSON serializable: {json_error}")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå API response test failed: {e}")
            return False
        
        # Summary
        logger.info("üèÅ End-to-End Workflow Test Complete")
        logger.info("‚úÖ All workflow components tested successfully:")
        logger.info("  1. ‚úÖ Frontend ‚Üí Backend message passing")
        logger.info("  2. ‚úÖ Assessment data bridge retrieval")
        logger.info("  3. ‚úÖ LLM prompt enhancement")
        logger.info("  4. ‚úÖ Response flattener processing")
        logger.info("  5. ‚úÖ Frontend-compatible output structure")
        
        return True

if __name__ == "__main__":
    success = asyncio.run(test_full_workflow())
    if success:
        print("\nüéØ OVERALL RESULT: ‚úÖ SUCCESS")
        print("The complete workflow from frontend to backend to LLM to frontend display is working correctly!")
    else:
        print("\n‚ùå OVERALL RESULT: FAILED")
        print("One or more workflow components need debugging.")