"""Google Cloud-integrated ADK Host for Educational IEP Generator"""

import os
import time
from typing import Dict, Any, List, Optional
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import httpx
from dotenv import load_dotenv

# Google Cloud imports
import vertexai
from vertexai.generative_models import GenerativeModel, Part, GenerationConfig, SafetySetting, HarmCategory, HarmBlockThreshold

# Load environment variables
load_dotenv()

# Configuration
class Settings:
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8002"))
    environment = os.getenv("ENVIRONMENT", "development")
    
    # Google Cloud settings
    gcp_project_id = os.getenv("GCP_PROJECT_ID", "thela002")
    gcp_region = os.getenv("GCP_REGION", "us-central1")
    gemini_model = os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
    
    # Service URLs
    mcp_server_url = os.getenv("MCP_SERVER_URL", "http://localhost:8006")
    auth_service_url = os.getenv("AUTH_SERVICE_URL", "http://localhost:8001")
    
    # AI Configuration
    mock_mode = os.getenv("MOCK_MODE", "false").lower() == "true"
    max_query_length = int(os.getenv("MAX_QUERY_LENGTH", "2000"))
    gemini_max_tokens = int(os.getenv("GEMINI_MAX_TOKENS", "2048"))
    gemini_temperature = float(os.getenv("GEMINI_TEMPERATURE", "0.7"))

settings = Settings()

# Global resources
http_client: Optional[httpx.AsyncClient] = None
gemini_model: Optional[GenerativeModel] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle"""
    global http_client, gemini_model
    
    # Initialize HTTP client
    http_client = httpx.AsyncClient(
        timeout=httpx.Timeout(30.0, connect=5.0),
        limits=httpx.Limits(
            max_keepalive_connections=20,
            max_connections=100
        )
    )
    
    # Initialize Vertex AI
    try:
        vertexai.init(
            project=settings.gcp_project_id,
            location=settings.gcp_region
        )
        gemini_model = GenerativeModel(settings.gemini_model)
        print(f"✅ Initialized Vertex AI Gemini model: {settings.gemini_model}")
        print(f"✅ Project: {settings.gcp_project_id}, Region: {settings.gcp_region}")
    except Exception as e:
        print(f"❌ Failed to initialize Vertex AI: {str(e)}")
        print("Will fall back to mock responses for development")
        settings.mock_mode = True
    
    yield
    
    # Shutdown
    if http_client:
        await http_client.aclose()

app = FastAPI(
    title="Educational IEP ADK Host (Google Cloud)",
    version="1.0.0",
    description="Backend-for-Frontend service with real Vertex AI Gemini integration",
    lifespan=lifespan
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://127.0.0.1:3000"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response models
class GenerateIEPRequest(BaseModel):
    student_id: str
    student_name: str
    grade_level: str
    disability_type: Optional[str] = None
    current_performance: Optional[str] = None
    goals: Optional[List[str]] = None
    additional_context: Optional[str] = None

class GenerateIEPResponse(BaseModel):
    iep_content: str
    sections: Dict[str, Any]
    metadata: Dict[str, Any]

class AnalyzeDocumentRequest(BaseModel):
    document_text: str
    analysis_type: str = "general"
    context: Optional[str] = None

class AnalyzeDocumentResponse(BaseModel):
    analysis: str
    key_points: List[str]
    recommendations: List[str]
    metadata: Dict[str, Any]

class QueryRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=2000)
    context: Optional[str] = None
    
# Frontend-compatible request format
class ADKQueryRequest(BaseModel):
    user_id: str
    query: str = Field(..., min_length=1, max_length=2000)
    context: Optional[Dict[str, Any]] = None
    parameters: Optional[Dict[str, Any]] = None

class QueryResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]] = []
    metadata: Dict[str, Any]

async def generate_with_gemini(prompt: str, context: Optional[str] = None) -> str:
    """Generate content using Vertex AI Gemini"""
    if settings.mock_mode or not gemini_model:
        return "Mock response: This would be generated by Vertex AI Gemini in production mode."
    
    try:
        # Combine context and prompt if provided
        full_prompt = prompt
        if context:
            full_prompt = f"Context: {context}\n\nQuestion/Task: {prompt}"
        
        # Generation configuration
        generation_config = GenerationConfig(
            temperature=settings.gemini_temperature,
            top_p=0.8,
            top_k=40,
            max_output_tokens=settings.gemini_max_tokens,
        )
        
        # Safety settings
        safety_settings = [
            SafetySetting(
                category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH
            ),
            SafetySetting(
                category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH
            ),
            SafetySetting(
                category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH
            ),
            SafetySetting(
                category=HarmCategory.HARM_CATEGORY_HARASSMENT,
                threshold=HarmBlockThreshold.BLOCK_ONLY_HIGH
            ),
        ]
        
        # Generate content
        response = gemini_model.generate_content(
            contents=[full_prompt],
            generation_config=generation_config,
            safety_settings=safety_settings,
        )
        
        if response.text:
            return response.text
        else:
            return "No response generated. Please try rephrasing your request."
            
    except Exception as e:
        print(f"Gemini generation error: {str(e)}")
        return f"Error generating response: {str(e)}"

# API v1 routes for frontend compatibility
@app.post("/api/v1/generate-iep", response_model=GenerateIEPResponse)
@app.post("/generate-iep", response_model=GenerateIEPResponse)  # Keep legacy endpoint
async def generate_iep(request: GenerateIEPRequest):
    """Generate IEP content using Vertex AI Gemini"""
    start_time = time.time()
    
    # Construct educational prompt for IEP generation
    prompt = f"""You are an expert special education professional tasked with creating an Individualized Education Program (IEP) for a student. Please generate comprehensive IEP content based on the following information:

Student Information:
- Name: {request.student_name}
- Grade Level: {request.grade_level}
- Disability Category: {request.disability_type or 'Not specified'}

Current Performance Level:
{request.current_performance or 'Assessment data needed to determine baseline performance levels.'}

Requested Goals:
{'; '.join(request.goals) if request.goals else 'Goals to be determined based on assessment data.'}

Additional Context:
{request.additional_context or 'No additional context provided.'}

Please provide a comprehensive IEP that includes:

1. Present Level of Academic Achievement and Functional Performance (PLAAFP)
2. Annual Goals (SMART goals - Specific, Measurable, Achievable, Relevant, Time-bound)
3. Special Education Services and Related Services
4. Accommodations and Modifications
5. Assessment Considerations
6. Transition Services (if age-appropriate)

Format the response as a structured IEP document with clear sections and professional language appropriate for educational settings."""

    try:
        iep_content = await generate_with_gemini(prompt)
        
        # Extract key sections for structured response
        sections = {
            "student_info": {
                "name": request.student_name,
                "grade": request.grade_level,
                "disability": request.disability_type
            },
            "present_level": request.current_performance or "Assessment needed",
            "goals": request.goals or ["To be determined based on assessment"],
            "services": ["Special education support", "Related services as needed"],
            "accommodations": ["To be determined based on individual needs"]
        }
        
        return GenerateIEPResponse(
            iep_content=iep_content,
            sections=sections,
            metadata={
                "generated_at": time.time(),
                "processing_time_ms": int((time.time() - start_time) * 1000),
                "model": settings.gemini_model,
                "student_id": request.student_id,
                "ai_provider": "vertex_ai_gemini"
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error generating IEP: {str(e)}"
        )

@app.post("/api/v1/analyze-document", response_model=AnalyzeDocumentResponse)
@app.post("/analyze-document", response_model=AnalyzeDocumentResponse)  # Keep legacy endpoint
async def analyze_document(request: AnalyzeDocumentRequest):
    """Analyze documents for educational insights using Vertex AI"""
    start_time = time.time()
    
    prompt = f"""You are an educational assessment specialist. Please analyze the following document and provide insights relevant to educational planning and IEP development.

Document Content:
{request.document_text}

Analysis Type: {request.analysis_type}
Context: {request.context or 'General educational analysis'}

Please provide:
1. A comprehensive analysis of the document's educational relevance
2. Key points that would be useful for educational planning
3. Specific recommendations for how this information could inform IEP development
4. Any assessment data or performance indicators identified

Focus on practical insights that would help special education professionals and teachers."""

    try:
        analysis = await generate_with_gemini(prompt)
        
        # Extract structured components (simplified for demo)
        key_points = [
            "Document contains educational assessment data",
            "Relevant for special education planning",
            "Contains performance indicators",
            "Applicable to IEP development process"
        ]
        
        recommendations = [
            "Incorporate findings into present level statements",
            "Use data to inform goal development",
            "Consider implications for service delivery",
            "Review for accommodation needs"
        ]
        
        return AnalyzeDocumentResponse(
            analysis=analysis,
            key_points=key_points,
            recommendations=recommendations,
            metadata={
                "document_length": len(request.document_text),
                "word_count": len(request.document_text.split()),
                "analysis_type": request.analysis_type,
                "processing_time_ms": int((time.time() - start_time) * 1000),
                "ai_provider": "vertex_ai_gemini"
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error analyzing document: {str(e)}"
        )

@app.post("/api/v1/query", response_model=QueryResponse)
async def process_adk_query(request: ADKQueryRequest):
    """Process educational queries using Vertex AI Gemini - Frontend compatible endpoint"""
    start_time = time.time()
    
    # Extract context information
    context_str = ""
    if request.context:
        context_parts = []
        if request.context.get("student_id"):
            context_parts.append(f"Student ID: {request.context['student_id']}")
        if request.context.get("grade_level"):
            context_parts.append(f"Grade Level: {request.context['grade_level']}")
        if request.context.get("section_type"):
            context_parts.append(f"Section Type: {request.context['section_type']}")
        if request.context.get("iep_context"):
            context_parts.append(f"IEP Context: {request.context['iep_context']}")
        context_str = "; ".join(context_parts) if context_parts else "General educational context"
    
    prompt = f"""You are an expert educational assistant specializing in special education, IEP development, and teaching strategies. Please provide a helpful, accurate, and practical response to the following question:

Question: {request.query}

Context: {context_str}

Please provide a comprehensive answer that includes:
- Clear, actionable information
- Best practices from special education
- Practical strategies for implementation
- Relevant educational considerations

Keep your response professional and suitable for educators, special education professionals, and administrators."""

    try:
        response_text = await generate_with_gemini(prompt, context_str)
        
        return QueryResponse(
            response=response_text,
            sources=[
                {
                    "title": "Vertex AI Gemini Knowledge Base",
                    "type": "AI_generated",
                    "relevance": 0.95,
                    "document_id": "gemini-knowledge",
                    "snippet": response_text[:200] + "..."
                }
            ],
            metadata={
                "query": request.query,
                "user_id": request.user_id,
                "processing_time_ms": int((time.time() - start_time) * 1000),
                "model": settings.gemini_model,
                "ai_provider": "vertex_ai_gemini",
                "context_provided": bool(request.context)
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing query: {str(e)}"
        )

@app.post("/query", response_model=QueryResponse)  # Keep legacy endpoint
async def process_query(request: QueryRequest):
    """Process educational queries using Vertex AI Gemini"""
    start_time = time.time()
    
    prompt = f"""You are an expert educational assistant specializing in special education, IEP development, and teaching strategies. Please provide a helpful, accurate, and practical response to the following question:

Question: {request.query}

Context: {request.context or 'General educational context'}

Please provide a comprehensive answer that includes:
- Clear, actionable information
- Best practices from special education
- Practical strategies for implementation
- Relevant educational considerations

Keep your response professional and suitable for educators, special education professionals, and administrators."""

    try:
        response_text = await generate_with_gemini(prompt, request.context)
        
        return QueryResponse(
            response=response_text,
            sources=[
                {
                    "title": "Vertex AI Gemini Knowledge Base",
                    "type": "AI_generated",
                    "relevance": 0.95
                }
            ],
            metadata={
                "query": request.query,
                "processing_time_ms": int((time.time() - start_time) * 1000),
                "model": settings.gemini_model,
                "ai_provider": "vertex_ai_gemini"
            }
        )
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing query: {str(e)}"
        )

@app.get("/api/v1/health")
async def health_check_v1():
    """Health check endpoint - Frontend compatible format"""
    # Check service statuses
    mcp_status = "disconnected"  # We don't have MCP server for now
    gemini_status = "connected" if gemini_model else "disconnected"
    vector_status = "connected"  # Assume connected for now
    
    # Check auth service
    if http_client:
        try:
            auth_response = await http_client.get(
                f"{settings.auth_service_url}/health",
                timeout=5.0
            )
            auth_status = "connected" if auth_response.status_code == 200 else "disconnected"
        except Exception:
            auth_status = "disconnected"
    else:
        auth_status = "disconnected"
    
    return {
        "status": "healthy" if gemini_status == "connected" else "degraded",
        "services": {
            "mcp_server": mcp_status,
            "gemini_api": gemini_status,
            "vector_store": vector_status,
            "auth_service": auth_status
        },
        "timestamp": time.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z',
        "ai_provider": "vertex_ai_gemini",
        "model": settings.gemini_model,
        "project": settings.gcp_project_id
    }

@app.get("/health")
async def health_check():
    """Health check endpoint - Legacy format"""
    health_status = {
        "status": "healthy",
        "service": "adk-host-gcp",
        "version": "1.0.0",
        "environment": settings.environment,
        "ai_provider": "vertex_ai_gemini",
        "model": settings.gemini_model,
        "project": settings.gcp_project_id,
        "region": settings.gcp_region
    }
    
    # Check Vertex AI availability
    if gemini_model:
        health_status["vertex_ai"] = {
            "status": "connected",
            "model": settings.gemini_model
        }
    else:
        health_status["vertex_ai"] = {
            "status": "disconnected",
            "fallback": "mock_mode" if settings.mock_mode else "unavailable"
        }
    
    # Check auth service connectivity
    if http_client:
        try:
            auth_response = await http_client.get(
                f"{settings.auth_service_url}/health",
                timeout=5.0
            )
            health_status["dependencies"] = {
                "auth_service": {
                    "status": "connected" if auth_response.status_code == 200 else "disconnected",
                    "status_code": auth_response.status_code
                }
            }
        except Exception as e:
            health_status["dependencies"] = {
                "auth_service": {
                    "status": "disconnected",
                    "error": str(e)
                }
            }
    
    return health_status

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "service": "Educational IEP ADK Host",
        "version": "1.0.0",
        "description": "Backend-for-Frontend service with Vertex AI Gemini integration",
        "ai_provider": "Google Cloud Vertex AI",
        "model": settings.gemini_model,
        "project": settings.gcp_project_id,
        "endpoints": {
            "generate_iep": "/api/v1/generate-iep",
            "analyze_document": "/api/v1/analyze-document", 
            "query": "/api/v1/query",
            "health": "/health",
            "legacy": {
                "generate_iep": "/generate-iep",
                "analyze_document": "/analyze-document",
                "query": "/query"
            }
        }
    }

if __name__ == "__main__":
    import uvicorn
    
    print(f"🚀 Starting Educational IEP ADK Host")
    print(f"📍 Port: {settings.port}")
    print(f"🌍 Environment: {settings.environment}")
    print(f"🤖 AI Provider: Google Cloud Vertex AI")
    print(f"🧠 Model: {settings.gemini_model}")
    print(f"📊 Project: {settings.gcp_project_id}")
    print(f"🌎 Region: {settings.gcp_region}")
    
    uvicorn.run(
        "gcp_main:app",
        host=settings.host,
        port=settings.port,
        reload=True
    )